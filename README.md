# JEMMA
JAX-based LLM inference

# TODO
gemma 2B
- [ ] Layernorm
- [ ] Dropout
- [ ] Causal multi-head attention
     - [ ] Multi-query attention 
- [ ] Positional embedding
- [ ] Tokenizer

Later
- [ ] FlashAttention (CUDA and Triton kernels)
- [ ] Weight Quantization
- [ ] OpenAI-compatible API